{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949e8556",
   "metadata": {},
   "source": [
    "# RNN Defect Detection per-Window Phase\n",
    "\n",
    "This notebook extends the defect detection model by computing amplitude and phase **for each sliding window** and incorporating these as additional window-level features into the RNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1afab379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Parameters ===\n",
    "TRAIN_DIR    = 'saved/train_data'\n",
    "TEST_CSV     = 'saved/test/test.csv'\n",
    "TEST_DIR    = 'saved/test'\n",
    "OUTPUT_DIR   = 'saved'\n",
    "DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "WINDOW_SIZES = [32, 64, 128]\n",
    "HIDDEN_SIZE  = 64\n",
    "NUM_LAYERS   = 2\n",
    "BATCH_SIZE   = 1024\n",
    "NUM_EPOCHS   = 10\n",
    "LR           = 1e-3\n",
    "\n",
    "CHANNEL_IDS  = [0, 1, 2]\n",
    "MODEL_PATH   = os.path.join(OUTPUT_DIR, 'rnn_defect_model.pth')\n",
    "SCALER_DIR   = os.path.join(OUTPUT_DIR, 'scalers')\n",
    "os.makedirs(SCALER_DIR, exist_ok=True)\n",
    "\n",
    "def get_amp_phase(data, position, width, lookup_size=100):\n",
    "    \"\"\"\n",
    "    Compute amplitude and phase for a window of 'data' centered at 'position' with given 'width'.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    p1 = max(0, position - width // 2)\n",
    "    p2 = min(n - 1, position + width // 2)\n",
    "    # find extremes on X and Y\n",
    "    A = B = C = D = p1\n",
    "    min_x = max_x = data[p1][0]\n",
    "    min_y = max_y = data[p1][1]\n",
    "    for i in range(p1+1, p2+1):\n",
    "        x,y = data[i]\n",
    "        if x < min_x: min_x, A = x, i\n",
    "        if x > max_x: max_x, B = x, i\n",
    "        if y < min_y: min_y, C = y, i\n",
    "        if y > max_y: max_y, D = y, i\n",
    "    if C < A: A,C = C,A\n",
    "    if D < B: B,D = D,B\n",
    "    if (C - A) > lookup_size:\n",
    "        tA = (A+C-lookup_size)//2\n",
    "        tC = (A+C+lookup_size)//2\n",
    "        A,C = tA,tC\n",
    "    if (D - B) > lookup_size:\n",
    "        tB = (B+D-lookup_size)//2\n",
    "        tD = (B+D+lookup_size)//2\n",
    "        B,D = tB,tD\n",
    "    # select farthest points\n",
    "    max_d = 0\n",
    "    q1 = q2 = p1\n",
    "    for s1 in range(A, C+1):\n",
    "        for s2 in range(B, D+1):\n",
    "            dx = data[s1][0] - data[s2][0]\n",
    "            dy = data[s1][1] - data[s2][1]\n",
    "            d = dx*dx + dy*dy\n",
    "            if d > max_d:\n",
    "                max_d, q1, q2 = d, s1, s2\n",
    "    x1,y1 = data[q1]\n",
    "    x2,y2 = data[q2]\n",
    "    amp = math.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "    phase = math.degrees(math.atan2(y2 - y1, x1 - x2)) % 360\n",
    "    return amp, phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b4a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scalers(train_dir, channel_ids):\n",
    "    \"\"\"Fit StandardScaler on the 6 per-timestep features for each channel.\"\"\"\n",
    "    scalers = {}\n",
    "    for ch in channel_ids:\n",
    "        cols = [f'{ch}_X', f'{ch}_Y', f'{ch}_dX', f'{ch}_dY', f'{ch}_ddX', f'{ch}_ddY']\n",
    "        all_feats = []\n",
    "        for path in glob.glob(os.path.join(train_dir, '*.csv')):\n",
    "            df = pd.read_csv(path)\n",
    "            # compute derivatives\n",
    "            df[f'{ch}_dX']  = df[f'{ch}_X'].diff().fillna(0)\n",
    "            df[f'{ch}_dY']  = df[f'{ch}_Y'].diff().fillna(0)\n",
    "            df[f'{ch}_ddX'] = df[f'{ch}_dX'].diff().fillna(0)\n",
    "            df[f'{ch}_ddY'] = df[f'{ch}_dY'].diff().fillna(0)\n",
    "            all_feats.append(df[cols].values.astype(np.float32))\n",
    "        all_feats = np.vstack(all_feats)\n",
    "        scaler = StandardScaler().fit(all_feats)\n",
    "        with open(os.path.join(SCALER_DIR, f'scaler_ch{ch}.pkl'), 'wb') as fp:\n",
    "            pickle.dump(scaler, fp)\n",
    "        scalers[ch] = scaler\n",
    "    return scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d8471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelDefectDataset(Dataset):\n",
    "    def __init__(self, train_dir, window_sizes, scalers, channel_ids):\n",
    "        self.window_sizes = window_sizes\n",
    "        self.scalers = scalers\n",
    "        self.channel_ids = channel_ids\n",
    "        self.items = []     # tuples of (feats_scaled, data_xy, labels)\n",
    "        self.idx_map = []   # (item_idx, row_idx)\n",
    "        for path in glob.glob(os.path.join(train_dir, '*.csv')):\n",
    "            df = pd.read_csv(path)\n",
    "            # compute derivatives\n",
    "            for ch in channel_ids:\n",
    "                df[f'{ch}_dX']  = df[f'{ch}_X'].diff().fillna(0)\n",
    "                df[f'{ch}_dY']  = df[f'{ch}_Y'].diff().fillna(0)\n",
    "                df[f'{ch}_ddX'] = df[f'{ch}_dX'].diff().fillna(0)\n",
    "                df[f'{ch}_ddY'] = df[f'{ch}_dY'].diff().fillna(0)\n",
    "            labels = (df['slice_number'].fillna(0).astype(int) != 0).astype(np.int64).values\n",
    "            for ch in channel_ids:\n",
    "                cols = [f'{ch}_X', f'{ch}_Y', f'{ch}_dX', f'{ch}_dY', f'{ch}_ddX', f'{ch}_ddY']\n",
    "                feats = df[cols].values.astype(np.float32)\n",
    "                feats = scalers[ch].transform(feats)\n",
    "                # data_xy needed for phase/amp\n",
    "                xy = list(zip(df[f'{ch}_X'], df[f'{ch}_Y']))\n",
    "                self.items.append((feats, xy, labels))\n",
    "        for item_idx, (_, _, labels) in enumerate(self.items):\n",
    "            for i in range(len(labels)):\n",
    "                self.idx_map.append((item_idx, i))\n",
    "    def __len__(self):\n",
    "        return len(self.idx_map)\n",
    "    def __getitem__(self, idx):\n",
    "        item_idx, row_idx = self.idx_map[idx]\n",
    "        feats, xy, labels = self.items[item_idx]\n",
    "        seqs = []\n",
    "        window_amps = []\n",
    "        window_phs  = []\n",
    "        for w in self.window_sizes:\n",
    "            start = row_idx - w + 1\n",
    "            if start >= 0:\n",
    "                window = feats[start:row_idx+1]\n",
    "                data_win = xy[start:row_idx+1]\n",
    "            else:\n",
    "                pad = np.zeros((-start, feats.shape[1]), dtype=np.float32)\n",
    "                window = np.vstack([pad, feats[0:row_idx+1]])\n",
    "                data_win = [(0,0)]*(-start) + xy[:row_idx+1]\n",
    "            amp, ph = get_amp_phase(data_win, len(data_win)-1, w)\n",
    "            seqs.append(window)\n",
    "            window_amps.append(amp)\n",
    "            window_phs.append(ph)\n",
    "        label = labels[row_idx]\n",
    "        return seqs, window_amps, window_phs, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be87d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch: list of (seqs, amps, phs, label)\n",
    "    #  - seqs: list of K windows, each window is (w, feat_dim)\n",
    "    #  - amps: list of K floats\n",
    "    #  - phs:  list of K floats\n",
    "    #  - label: scalar\n",
    "\n",
    "    # 1) stack sequences per window\n",
    "    seqs_list = list(zip(*[b[0] for b in batch]))  \n",
    "    #    seqs_list[i] is a tuple of length batch, each element is (w,feat_dim)\n",
    "    seqs_tensors = [\n",
    "        torch.tensor(np.stack(win_batch), dtype=torch.float32)\n",
    "        for win_batch in seqs_list\n",
    "    ]  # list of K tensors, each (batch, w, feat_dim)\n",
    "\n",
    "    # 2) build (batch, K) tensors for amps and phs\n",
    "    amps = torch.tensor([b[1] for b in batch], dtype=torch.float32)  # shape (batch, K)\n",
    "    phs  = torch.tensor([b[2] for b in batch], dtype=torch.float32)  # shape (batch, K)\n",
    "\n",
    "    # 3) labels\n",
    "    labels = torch.tensor([b[3] for b in batch], dtype=torch.float32)  # (batch,)\n",
    "\n",
    "    return seqs_tensors, amps, phs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ff9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleRNNWithPhase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, window_sizes, num_layers):\n",
    "        super().__init__()\n",
    "        self.window_sizes = window_sizes\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                    batch_first=True, dropout=0.2)\n",
    "            for _ in window_sizes\n",
    "        ])\n",
    "        # each window: hidden + [amp, phase] => hidden+2\n",
    "        total_feats = len(window_sizes)*(hidden_size+2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(total_feats, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    def forward(self, seqs_list, amps, phs):\n",
    "        hs = []\n",
    "        for i, lstm in enumerate(self.branches):\n",
    "            seq = seqs_list[i]\n",
    "            _, (h_n, _) = lstm(seq)\n",
    "            h_last = h_n[-1]                 # (batch, hidden_size)\n",
    "            amp_i = amps[:, i].unsqueeze(1)  # (batch,1)\n",
    "            ph_i  = phs[:, i].unsqueeze(1)\n",
    "            hs.append(torch.cat([h_last, amp_i, ph_i], dim=1))\n",
    "        h = torch.cat(hs, dim=1)\n",
    "        return self.classifier(h).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea5bfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 594/594 [00:48<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 0.1307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 594/594 [00:45<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 594/594 [00:45<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 594/594 [00:44<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss: 0.0262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 594/594 [00:45<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss: 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 594/594 [00:45<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 594/594 [00:45<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 594/594 [00:45<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 594/594 [00:45<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 594/594 [00:45<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss: 0.0195\n"
     ]
    }
   ],
   "source": [
    "# === Training ===\n",
    "scalers = fit_scalers(TRAIN_DIR, CHANNEL_IDS)\n",
    "ds = MultiChannelDefectDataset(TRAIN_DIR, WINDOW_SIZES, scalers, CHANNEL_IDS)\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "model = MultiScaleRNNWithPhase(input_size=6, hidden_size=HIDDEN_SIZE,\n",
    "                               window_sizes=WINDOW_SIZES, num_layers=NUM_LAYERS).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_loss = float('inf')\n",
    "for ep in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for seqs, amps, phs, labels in tqdm(loader, desc=f'Epoch {ep}'):\n",
    "        seqs = [s.to(DEVICE) for s in seqs]\n",
    "        amps = amps.to(DEVICE)\n",
    "        phs  = phs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = model(seqs, amps, phs)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "    avg = total_loss / len(ds)\n",
    "    print(f'Epoch {ep}, Avg Loss: {avg:.4f}')\n",
    "    if avg < best_loss:\n",
    "        best_loss = avg\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47950b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results saved to saved/test_out.csv\n"
     ]
    }
   ],
   "source": [
    "# === Inference ===\n",
    "import pickle\n",
    "\n",
    "# Load scalers\n",
    "scalers = {ch: pickle.load(open(os.path.join(SCALER_DIR, f'scaler_ch{ch}.pkl'),'rb'))\n",
    "           for ch in CHANNEL_IDS}\n",
    "\n",
    "# Load model\n",
    "model = MultiScaleRNNWithPhase(input_size=6, hidden_size=HIDDEN_SIZE,\n",
    "                               window_sizes=WINDOW_SIZES, num_layers=NUM_LAYERS).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# Read test data\n",
    "df = pd.read_csv(TEST_CSV)\n",
    "# compute derivatives\n",
    "for ch in CHANNEL_IDS:\n",
    "    df[f'{ch}_dX']  = df[f'{ch}_X'].diff().fillna(0)\n",
    "    df[f'{ch}_dY']  = df[f'{ch}_Y'].diff().fillna(0)\n",
    "    df[f'{ch}_ddX'] = df[f'{ch}_dX'].diff().fillna(0)\n",
    "    df[f'{ch}_ddY'] = df[f'{ch}_dY'].diff().fillna(0)\n",
    "\n",
    "# prepare per-window data and infer\n",
    "n = len(df)\n",
    "for ch in CHANNEL_IDS:\n",
    "    # get scaled feats and xy\n",
    "    cols = [f'{ch}_X', f'{ch}_Y', f'{ch}_dX', f'{ch}_dY', f'{ch}_ddX', f'{ch}_ddY']\n",
    "    feats = df[cols].values.astype(np.float32)\n",
    "    feats = scalers[ch].transform(feats)\n",
    "    xy    = list(zip(df[f'{ch}_X'], df[f'{ch}_Y']))\n",
    "    proba = np.zeros(n, dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, n, BATCH_SIZE):\n",
    "            end = min(n, start + BATCH_SIZE)\n",
    "            batch_size = end - start\n",
    "            seqs, amps, phs = [], [], []\n",
    "            for w in WINDOW_SIZES:\n",
    "                # build batch window matrix\n",
    "                mat = np.zeros((batch_size, w, 6), dtype=np.float32)\n",
    "                batch_amps = []\n",
    "                batch_phs  = []\n",
    "                for i in range(batch_size):\n",
    "                    idx = start + i\n",
    "                    s = idx - w + 1\n",
    "                    if s >= 0:\n",
    "                        window = feats[s:idx+1]\n",
    "                        data_win = xy[s:idx+1]\n",
    "                    else:\n",
    "                        pad = np.zeros((-s,6), dtype=np.float32)\n",
    "                        window = np.vstack([pad, feats[:idx+1]])\n",
    "                        data_win = [(0,0)]*(-s) + xy[:idx+1]\n",
    "                    mat[i] = window\n",
    "                    amp, ph = get_amp_phase(data_win, len(data_win)-1, w)\n",
    "                    batch_amps.append(amp)\n",
    "                    batch_phs.append(ph)\n",
    "                seqs.append(torch.tensor(mat, dtype=torch.float32).to(DEVICE))\n",
    "                amps.append(batch_amps)\n",
    "                phs.append(batch_phs)\n",
    "            # transpose amps/phs to shape (batch, num_windows)\n",
    "            amps_t = torch.tensor(np.stack(amps, axis=1), dtype=torch.float32).to(DEVICE)\n",
    "            phs_t  = torch.tensor(np.stack(phs, axis=1), dtype=torch.float32).to(DEVICE)\n",
    "            logits = model(seqs, amps_t, phs_t).cpu().numpy()\n",
    "            proba[start:end] = 1/(1+np.exp(-logits))\n",
    "    df[f'defect_proba_{ch}'] = proba\n",
    "\n",
    "# Save results\n",
    "out_csv = os.path.join(OUTPUT_DIR, os.path.basename(TEST_CSV).replace('.csv','_out.csv'))\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f'Inference results saved to {out_csv}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9f3b06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 594/594 [00:47<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9927\n",
      "Precision: 0.9837\n",
      "Recall   : 0.7119\n",
      "F1 Score : 0.8260\n",
      "ROC AUC  : 0.9969\n"
     ]
    }
   ],
   "source": [
    "# === Evaluation Metrics ===\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Prepare your test DataLoader\n",
    "test_ds     = MultiChannelDefectDataset(TRAIN_DIR, WINDOW_SIZES, scalers, CHANNEL_IDS)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "model = MultiScaleRNNWithPhase(\n",
    "    input_size=6,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    window_sizes=WINDOW_SIZES,\n",
    "    num_layers=NUM_LAYERS\n",
    ").to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_probs  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seqs, amps, phs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "        # move inputs to device one-by-one for seqs (a list of tensors)\n",
    "        seqs = [win.to(DEVICE) for win in seqs]\n",
    "        amps = amps.to(DEVICE)\n",
    "        phs  = phs.to(DEVICE)\n",
    "\n",
    "        logits = model(seqs, amps, phs)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "        \n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels.numpy().flatten())\n",
    "\n",
    "# compute metrics\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds  = (np.array(all_probs) > 0.5).astype(int)\n",
    "\n",
    "accuracy  = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall    = recall_score(all_labels, all_preds)\n",
    "f1        = f1_score(all_labels, all_preds)\n",
    "roc_auc   = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "print(f'Accuracy : {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall   : {recall:.4f}')\n",
    "print(f'F1 Score : {f1:.4f}')\n",
    "print(f'ROC AUC  : {roc_auc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
